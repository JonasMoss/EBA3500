---
title: "Exercises for Lecture 8: Binary regression"
subtitle: "EBA3500 Data analysis with programming"
author: "Jonas Moss"
institute: BI Norwegian Business School </br> Department of Data Science and Analytics
format: 
    html:
      self-contained: true
editor: visual
execute:
  cache: false
  enabled: true
  output: true
  freeze: auto
jupyter: python3
number-sections: true
---

**Updated 17/11/2022: Fixed error in the exercise about the exponential distribution; clarified two points about the uniform distribution.**

## Maximum likelihood

Recall the definition of the maximum likelihood estimator. If $X_1,X_2,\ldots, X_n\sim(\theta)$ are iid for some unknown $\theta$, the maximum likelihood estimator is $$\hat{\theta} =\operatorname{argmax}_\theta \sum_{i=1}^n \log f(X_i;\theta),$$ where $\theta$ can be either a scalar or a vector.

### Bernoulli distribution

> Let $X_1, ..., X_n$ be independent $0-1$ variables with success probability $p$, i.e., Bernoulli variables. Use differentiation to show that the maximum likelihood estimator of $p$ is $\hat p = \overline{X}$, the mean of $X_1, ..., X_n$. (*Hint*: Find the expression for the likelihood in the lecture slides or online. Google "differentiation maximization" or something if you have forgotten how to optimize using differentiation.)

You can find the solution [here](https://stats.stackexchange.com/questions/275380/maximum-likelihood-estimation-for-bernoulli-distribution).

### Normal distribution

> I showed in class that $\hat{\mu}_{ML} = \overline{x}$, the empirical mean, when the observations $x$ are sampled from a normal distribution $N(\mu,\sigma)$. Try to show this yourself! \[...\] Show that the maximum likelihood estimator of $\sigma$ is $\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}}$. (*Hint:* You must use the expression for the maximum likelihood of $\mu$ that you have already derived to derive the maximum likelihood estimator of $\sigma^2$. Then use the [invariance principle](https://stats.stackexchange.com/a/77661/179305).)

See e.g. [this site](https://online.stat.psu.edu/stat415/lesson/1/1.2).

### The exponential distribution

#### Maximum likelihood estimator of $\lambda$

> Let $X_1,X_2,\ldots,X_n$ be iid from an exponential distribution with density $f(x; \lambda) = \lambda\exp(-\lambda x)$. Calculate the maximum likelihood estimator of $\lambda$. What is the maximum likelihood estimator of $\sin(\lambda)$?

The maximum likelihood estimator is $1/\overline{x}$. The maximum likelihood estimator of $\sin(\lambda)$ is $\sin(1/\overline{x})$ by the invariance principle.

#### Asymptotics of $\hat{\lambda}$

> Make a Python function that samples $n$ exponentials with parameter $\lambda = 2$ and calculates its maximum likelihood estimator. Make a histogram out of $N = 10,000$ samples from this function when $n = 100$. What does the histogram look like?

```{python}
import numpy as np
import seaborn as sns
rng = np.random.default_rng(seed = 313)
samples = 1/rng.exponential(size = (10000, 100), scale = 2).mean(axis = 1)
sns.histplot(samples, bins = 100, stat = "density")
```

This histogram looks normal.

#### Rescaling the histogram

> Rescale the histogram, i.e., use $\sqrt{n}(\hat{\lambda}_{ML} -\lambda)$, and display it for $n = 100, 1000, 10000$. What do you see?

```{python}
def sample(n):
  theta_hats = 1/rng.exponential(size = (10000, n), scale = 1/2).mean(axis = 1)
  return np.sqrt(n) * (theta_hats - 2)
sns.histplot(sample(100), bins = 100, stat = "density")
sns.histplot(sample(1000), bins = 100, stat = "density")
sns.histplot(sample(10000), bins = 100, stat = "density")
```

The distributions are more or less the same, and they look normal.

#### The asymptotic variance (i)

Use the function in (b) to estimate the *asymptotic variance* $n\operatorname{Var}(\hat{\theta}_{ML})$ as $n$ varies. What do you see?

```{python}
[sample(n).var() for n in [100, 200, 1000, 5000, 10000]]
```

The variances stabilizes at $4$!

#### The asymptotic variance (ii)

> 1.  Calculate $\frac{\partial^2}{\partial\lambda^2}\log f(X;\lambda)$, i.e., the second derivative of the log-density of $X$ with respect to $\lambda$. (*Hint*: Take the logarithm first, then differentiate with respect to $\lambda$ twice!)

$\log (\lambda\exp[-\lambda x]) = \log(\lambda) - \lambda x$, so its derivative is $1/\lambda-x$ and second derivative $-1/\lambda^2$.

> 2.  Calculate the expectation of the expression you just found, $I(\lambda)=-E\left[\frac{\partial^{2}}{\partial\lambda^{2}}\log f(X;\lambda)\right]$

Its expectation is $1/\lambda^2$

> 3.  What is $1/I(\lambda)$ when evaluated in $\lambda = 2$? Do you recognize it?

It's $\lambda^2=4$, which equals the asymptotic variance we found above.

### The uniform distribution

The uniform distribution on $[0, b]$ has density $$f(x;b)=\begin{cases}
\frac{1}{b} & 0\leq x\leq b,\\
0 & \text{otherwise}.
\end{cases}$$

Let $X_1,X_2,\ldots X_n\sim f(x;b)$.

> 1.  Verify that $f(x;b)$ is a density.

The indefinite integral is $\int_1^b 1/b dx = x/b$. Now plug in the limits.

> 2.  Calculate the expectation of $X \sim f(x, b)$. Can you imagine a reasonable estimator of $b$?

The expectation is $b/2$ as $E(X)=\int_0^b x dx /b= b^2/(2b) = b/2$. When you plug in the limits. A reasonable estimator for $b$ is $2E(X)$.

> 3.  Try to show that the maximum likelihood estimator of $b$ is $\max_{i=1}^n x_i$. (*Hint:* Don't use differentiation!)

The smaller the upper limit $b$ is, the higher the likelihood, as long as every observation is smaller than or equal to $b$. (If $x>b$ the likelihood is $0$!) It follows that $\hat{b} = \max_{i=1}^n x_i$. See e.g. [this link](https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/lecture2.pdf), p. 13, for more details and explanation.

> 4.  Compare the estimator you derived in (2) to the maximum likelihood estimator using the same techniques as in the exercise on the exponential distribution. But you might have to multiply with $n$ instead of $\sqrt{n}$ for the histograms to stabilize! Which estimator do you prefer, and why?

Let's look at the mean times 2:

```{python}
def means(n):
  theta_hats = rng.uniform(size = (10000, n), low = 0, high = 2).mean(axis = 1)
  return np.sqrt(n) * (2*theta_hats - 2)
sns.histplot(means(100), bins = 100, stat = "density")
sns.histplot(means(1000), bins = 100, stat = "density")
sns.histplot(means(10000), bins = 100, stat = "density")
```

And the maximum, rescaled by $n$.

```{python}
def maxs(n):
  theta_hats = rng.uniform(size = (10000, n), low = 0, high = 2).max(axis = 1)
  return n * (2 - theta_hats)
sns.histplot(maxs(100), bins = 100, stat = "density")
sns.histplot(maxs(1000), bins = 100, stat = "density")
sns.histplot(maxs(10000), bins = 100, stat = "density")
```

> ***N.B.*** This maximum likelihood estimator is *irregular* since it does not follow a normal distribution, as you can observe from the histogram.

Observe that the distribution is exponential, not normal!

The asymptotic variance of the second estimator is much, much smaller, as it converges to $0$ with the rate $n^{-1}$ instead of $1/\sqrt{n}$!

For instance,

```{python}
obs = rng.uniform(size = (10000, 100), low = 0, high = 2)
obs.mean(axis=1).var()
obs.max(axis=1).var()
```
