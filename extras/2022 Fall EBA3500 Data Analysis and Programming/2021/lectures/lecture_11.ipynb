{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXdeN6tD7Z7S"
   },
   "source": [
    "# EBA3500 Lecture 11. Expectation, bias, and the adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "1. Review of expectation and variance\n",
    "2. Estimators\n",
    "3. Unbiased estimators and the adjusted $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of expectation and variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation\n",
    "Recall the expectation $E(X)$, which equals the theoretical mean of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Expectation)\n",
    "> Let $X$ be a discrete random variable with probability mass function $p(x) = P(X=x)$. Then $E(X) = \\sum xp(x)$ is the *expectation* or *expected value* of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If $X$ isn't discrete, which is usually the case, we would have to use the integral instead of the sum, i.e., $E(X) = \\int xp(x)dx$. There is *no difference* in interpretation though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Let $X$ be Bernoulli distributed with success probability $\\pi$, i.e., $p(X=1) = \\pi$ and $p(X=0) = 1 - \\pi$. Then \n",
    "$$p(X=1)\\cdot1 + p(X=0)\\cdot 0 = \\pi\\cdot1 + (1-\\pi) \\cdot 0 = \\pi,$$\n",
    "hence $E(X) = \\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation operator has a  momentous property, namely that of being linear. This property allows us to calculate stuff without using the definition of the expectation given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposition: Linearity of expectation\n",
    "> Let $X_1, X_2$ be random variables and $a, b$ be numbers. Then\n",
    "$E(aX_1 + bX_2) = aEX_1+bEX_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, function is linear if it acts like this, for instance $f(ax+by) = af(x) + bf(y)$ where $a,b$ are numbers and $x,y$ are some kind of mathematical objects. For instance, matrix multiplication is linear, $A(ax+by) = aA(x) + bA(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Suppose $X_1$ and $X_2$ are Bernoulli variables with parameters $\\pi_1$ and $\\pi_2$. What is the expectation of $X_1 + X_2$? \n",
    "Using the linearity of expectation, we find that $\\pi_1 + \\pi_2$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "The variance of a random variable $X$ captures its dispersion, or how spread out it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Variance)\n",
    "> Let $X$ be a random variable. Then $$\\textrm{Var}(X) = E(X^2) - E(X)^2$$ is the *variance* or of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example (Bernoulli)\n",
    "Let $X$ be Bernoulli distributed with success probability $\\pi$, i.e., $p(X=1) = \\pi$ and $p(X=0) = 1 - \\pi$. Then $E(X) = \\pi$, so we can calculate the $E(X)^2 = \\pi^2$ part of the variance equation. To calculate the $EX^2$ part, use\n",
    "$$p(X=1)\\cdot1^2 + p(X=0)\\cdot 0^2 = \\pi\\cdot1^2+ (1-\\pi) \\cdot 0^2 = \\pi.$$\n",
    "Hence $\\textrm{Var}(X) = \\pi - \\pi^2 = \\pi(1-\\pi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators\n",
    "Let $\\theta$ be some population value, e.g., an expectation $E(X)$ of some sort, or a regression coefficient. This value is typically unknown. An estimator $\\hat{\\theta}_n$ is a statistical measurement, based on observed data, of this population value. Whenever we say a population value, think about input to the data-generating process: These are the exact values the creator of a simulation study decides on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Estimator) ([Wikipedia source](https://en.wikipedia.org/wiki/Estimator))\n",
    "> In statistics, an estimator is a rule for calculating an estimate of a given quantity based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "1. Let $Y \\sim \\beta_0 + \\beta_1 X + \\epsilon$ and $\\hat{\\beta_1}$ be the regression coefficient estimated by least squares or least absolute deviations. Then $\\hat{\\beta_1}$ is an estimator of $\\beta_1$, and $\\beta_1$ is the estimand of $\\hat{\\beta_1}$.\n",
    "2. Let $x = x_1, x_2, ..., x_n$ be some observed data. Then the mean (i.e., `np.mean(x)`) of $x$ is an estimator of the theoretical population mean $\\mu$. ($\\mu$ is the true expectation.)\n",
    "3. Suppose we have a regression model with $p$ covariates and calculate its $R^2$. Then $R^2$ is an estimator of the true population $R^2$. \n",
    "4. Let, again, $x = x_1, x_2, ..., x_n$ be some observed data. Then the median (i.e., `np.median(x)`) of $x$ is an estimator of the theoretical population median. \n",
    "5. Is a *p*-value an estimator? No, as it does not attempt to measure a population value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical quantities\n",
    "The theoretical quantities, such as the population mean, population median, and population $R^2$, are quite hard to understand. Many people never understand them at all, but you are different! ðŸ˜Š They are properties of the data-generating process. Recall the intuition behind the data generating process. Alice, or Vishnu, or any other entity, has a program that generates your observed data. If you *knew* that program, you would have been able to calculate anything you'd ever want to. But you don't, and that's why you need to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.63182242, 0.46427464, 0.77927765, 0.40253182, 0.61196237,\n",
       "        0.32392294, 0.10567386, 0.68671495, 0.01786417, 0.60813899]),\n",
       " array([0.4591469 , 0.767279  , 0.24938788, 0.90998112, 0.49108449,\n",
       "        1.12724964, 2.24739775, 0.37583599, 4.02495832, 0.49735181])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is Alice who simulates for Bob. Bob never sees the program.\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(seed = 313)\n",
    "u = rng.random(10) # uniformly distributed variables on [0,1]\n",
    "x = -np.log(u)\n",
    "[u, x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random variable $U$ is uniform, and has theoretical mean (expectation) $1/2$. Bob doesn't know what Alice is doing though, so he has to estimate the mean. The random variable $X$ also have an expectation, namely $1$. (If you know integral calculus, you can calculate this yourself!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusted $R^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too big $R^2$ values\n",
    "The $R^2$ is good for evaluating how well be can predict the outcome given our covariates, but it's not good for choosing between models. In a nutshell, it's not good for choosing between models since it doesn't correct for the bias that occurs when using the same data both to estimate the model parameters and evaluating model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(seed = 313)\n",
    "p = 10\n",
    "n = 100\n",
    "x = rng.normal(0, 1, (n, p))\n",
    "y = rng.normal(3, 2, n)\n",
    "# think about y ~ 1 + x1 + x2 + ... + x_p\n",
    "# True R^2 is 0!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true $R^2$ for `y ~ x` is $0$ in this model. However, its expectation is not. Recall that \n",
    "$$R^2 \\sim \\textrm{Beta}\\left(\\frac{k-1}{2},\\frac{n-k}{2}\\right),$$\n",
    "from lecture 8, where $k$ is the number of parameters including the intercept. (That is, $k = p + 1$.) Using the properties of the $\\beta$ distribution, we find that the expected value of $R^2$ is $$E(R^2) = \\frac{\\frac{k-1}{2}}{\\frac{k-1}{2} + \\frac{n-k}{2}} = \\frac{k-1}{n - 1}.$$\n",
    "\n",
    "This number might be quite large when the number of predictors is close to the number of observations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10158243551749126, 0.10101010101010101]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "n_reps = 5000\n",
    "yy = rng.normal(3, 2, (n_reps, n))\n",
    "\n",
    "def func1d(y):\n",
    "  fit = smf.ols(\"y ~ x\", data = {}).fit() # y ~ x1 + x2 + ... + x_p\n",
    "  return fit.rsquared\n",
    "\n",
    "rsqs = np.apply_along_axis(\n",
    "    func1d = func1d,\n",
    "    axis = 1,\n",
    "    arr = yy\n",
    ")\n",
    "\n",
    "[np.mean(rsqs), p / (n-1)] # Since k = p + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Unbiased estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with $R^2$ is that it's *positively biased*, its expected value is larger than its true value. Our goal is to modify the $R^2$ so that it's approximately unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition (Unbiased estimator)\n",
    "> An estimator is *unbiased* if $E(\\hat{\\theta}_n) = \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some estimators are unbiased, such as the sample mean $\\frac{1}{n}\\sum_i x_i$, but most popular estimators are not unbiased, and it is *not* an important property in most scenarios. For instance, the estimated regression coefficients in a logistic regression are not unbiased. Neither are the estimated regression coefficients when using least absolute deviations. However, the sampled variance $S^2 = \\frac{1}{n-1}\\sum_i (X_i - \\overline{X})^2$ is unbiased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Constructing the adjusted $R^2$\n",
    "$$R^2 = 1 - \\frac{\\textrm{Sum of squares with predictors}}{\\textrm{Sum of squares without predictors}}$$\n",
    "\n",
    "We can show that $\\textrm{Sum of squares with predictors}$ is biased for its population value, the true sum of squares with predictors at our estimated regression coefficient. But we can correct for this bias! One can show that $$\\frac{n}{n-p-1}E(\\textrm{Sum of squares with predictors})$$ equals the true population sum of squares with predictors. Here $p$ is the number of estimated regression coefficinets, minus the intercept. Moreover, we can show that \n",
    "$$\\frac{n}{n-1} E(\\textrm{Sum of squares with predictors})$$ \n",
    "equals the true, population value of the sum of squares without predictors.\n",
    "\n",
    "It follows that a reasonable corrected $R^2$ is\n",
    "$$R_a^2 = 1 - \\frac{\\frac{n}{n-p-1} \\textrm{Sum of squares with predictors}}{\\frac{n}{n-1}\\textrm{Sum of squares without predictors}}$$\n",
    "\n",
    "Rearranging this, we find that\n",
    "$$ R_a ^ 2 = 1 - (1 - R^2) \\frac{n-1}{n-p-1} .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "1. The adjusted $R^2$, or $R^2_a$, can be less than $0$. Try to understand why. \n",
    "2. We haven't proved that the adjusted $R^2$ squared is unbiased for the true $R^2$. Do you think it is? \n",
    "3. Can you devise a simulation study to explore this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. The expectation of a random variable $X$ is denoted by $E(X)$, and equals the theorotical mean of random variable $X$.\n",
    "1. The variance of a random variable $X$ is denoted by $\\textrm{Var}(X)$, and captures the expected dispersion of $X$.\n",
    "2. An estimator approximates a population value based on observed data.\n",
    "4. An estimator $\\hat{\\theta}_n$ is *unbiased* if it equals $\\theta$ in expectation, i.e., $E[\\hat{\\theta}_n]=\\theta$.\n",
    "5. Unbiased estimation is not important, but it makes sense to correct the $R^2$ for bias.\n",
    "6. One attempt at bias-corrected $R^2$ is the adjusted $R^2$."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8214f72d23884c802e2dc6bec8e87b93e644d7518e94910628cfbca0d96cb336"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
